from fastapi import APIRouter, HTTPException, File, UploadFile
from app.services.ai_analyzer import AIAnalyzer
from app.services.document_parser import DocumentParser
from app.core.config import settings
from pydantic import BaseModel
from typing import Dict, Any, Optional
import logging

logger = logging.getLogger(__name__)
router = APIRouter()

# Initialize AI analyzer and document parser
ai_analyzer = AIAnalyzer()
document_parser = DocumentParser()

# Request models
class JobDescriptionRequest(BaseModel):
    description: str
    title: Optional[str] = None
    company: Optional[str] = None
    enhance_with_ai: Optional[bool] = True  # üåü NEW: Enable AI enhancement by default

class JobEnhancementRequest(BaseModel):
    """üåü NEW: Dedicated endpoint for job description enhancement"""
    description: str
    title: Optional[str] = None

class ResumeTextRequest(BaseModel):
    resume_text: str
    job_context: Optional[Dict] = None

@router.post("/enhance-job-description")
async def enhance_job_description(
    request: JobEnhancementRequest
) -> Dict[str, Any]:
    """
    üåü NEW: AI-powered job description enhancement
    
    This endpoint:
    1. Takes raw job description input from user
    2. Uses Google Gemini to clean, structure, and optimize it
    3. Returns enhanced description with extracted requirements
    4. Provides quality improvement metrics
    """
    
    logger.info(f"üîß Enhancing job description: {request.title or 'Untitled Job'}")
    
    try:
        if not request.description.strip():
            raise HTTPException(
                status_code=400,
                detail="Job description cannot be empty"
            )
        
        # Enhance with Google Gemini
        enhancement_result = await ai_analyzer.enhance_job_description(
            request.description, 
            request.title
        )
        
        result = {
            "status": "success",
            "enhancement": enhancement_result,
            "ai_provider": "google_gemini",
            "features": ["grammar_correction", "skill_standardization", "requirement_extraction", "structure_optimization"],
            "message": "Job description enhanced successfully with AI optimization"
        }
        
        # Add quality improvement metrics
        if "quality_score" in enhancement_result:
            result["improvement_metrics"] = {
                "original_quality": "estimated_from_input",
                "enhanced_quality": enhancement_result["quality_score"],
                "improvement_areas": enhancement_result.get("optimization_notes", ""),
                "was_enhanced": True
            }
        
        logger.info("‚úÖ Job description enhancement completed successfully")
        return result
        
    except Exception as e:
        logger.error(f"‚ùå Error enhancing job description: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Error enhancing job description: {str(e)}"
        )

@router.post("/analyze-job")
async def analyze_job_description(
    request: JobDescriptionRequest
) -> Dict[str, Any]:
    """
    Enhanced job description analysis with optional AI optimization
    
    This endpoint:
    1. Optionally enhances the job description first (if enhance_with_ai=True)
    2. Uses Google Gemini to extract structured requirements
    3. Returns detailed analysis with skill categorization
    4. Includes enhancement details if optimization was used
    """
    
    logger.info(f"üìä Analyzing job description: {request.title or 'Untitled Job'}")
    
    try:
        if not request.description.strip():
            raise HTTPException(
                status_code=400,
                detail="Job description cannot be empty"
            )
        
        # Analyze with optional enhancement
        analysis = await ai_analyzer.analyze_job_description(
            request.description,
            request.title,
            use_enhancement=request.enhance_with_ai
        )
        
        result = {
            "status": "success",
            "job_title": request.title,
            "company": request.company,
            "analysis": analysis,
            "ai_provider": "google_gemini",
            "features": ["enhanced_skill_categorization", "requirement_extraction", "experience_analysis"],
            "message": f"Job description analyzed successfully {'with AI enhancement' if request.enhance_with_ai else 'without enhancement'}"
        }
        
        # Add enhancement information if available
        if "enhancement_details" in analysis:
            result["enhancement_info"] = {
                "was_enhanced": analysis["enhancement_details"]["was_enhanced"],
                "optimization_applied": request.enhance_with_ai
            }
            
            if analysis["enhancement_details"]["was_enhanced"]:
                result["enhancement_info"]["improvements"] = analysis["enhancement_details"].get("optimization_notes")
                result["enhancement_info"]["quality_boost"] = analysis["enhancement_details"].get("quality_improvement")
        
        logger.info("‚úÖ Enhanced job analysis completed successfully")
        return result
        
    except Exception as e:
        logger.error(f"‚ùå Error analyzing job description: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Error analyzing job description: {str(e)}"
        )

@router.post("/analyze-job-file")
async def analyze_job_description_file(
    file: UploadFile = File(...),
    enhance_with_ai: bool = True
) -> Dict[str, Any]:
    """
    Upload and analyze a job description file (PDF, DOC, DOCX)

    This endpoint:
    1. Validates and parses the uploaded job description file
    2. Extracts text content from PDF/Word documents
    3. Optionally enhances with AI optimization
    4. Analyzes and extracts structured requirements
    5. Returns detailed job analysis with skill categorization
    """

    logger.info(f"üìÑ Analyzing job description file: {file.filename}")

    try:
        # Validate file type
        if not file.filename.lower().endswith(('.pdf', '.doc', '.docx')):
            raise HTTPException(
                status_code=400,
                detail="Unsupported file format. Please upload PDF, DOC, or DOCX files."
            )

        # Validate file size (20MB limit for job descriptions)
        if not document_parser.validate_file_size(file, max_size_mb=20):
            raise HTTPException(
                status_code=400,
                detail="File too large. Maximum size is 20MB for job descriptions."
            )

        # Parse document to extract text
        logger.info(f"üîç Parsing document: {file.filename}")
        job_description_text = await document_parser.parse_document(file)

        if not job_description_text.strip():
            raise HTTPException(
                status_code=400,
                detail="No text content found in the uploaded file."
            )

        # Extract potential job title from first few lines
        lines = job_description_text.split('\n')
        potential_title = None
        for line in lines[:5]:  # Check first 5 lines
            line = line.strip()
            if line and len(line) < 100 and any(keyword in line.lower() for keyword in
                ['developer', 'engineer', 'analyst', 'manager', 'specialist', 'lead', 'senior', 'junior']):
                potential_title = line
                break

        # Use extracted title or filename as fallback
        job_title = potential_title or file.filename.rsplit('.', 1)[0]

        logger.info(f"üìä Extracted job title: {job_title}")
        logger.info(f"üìù Job description length: {len(job_description_text)} characters")

        # Analyze with AI (with optional enhancement)
        analysis = await ai_analyzer.analyze_job_description(
            job_description_text,
            job_title,
            use_enhancement=enhance_with_ai
        )

        result = {
            "status": "success",
            "filename": file.filename,
            "extracted_job_title": job_title,
            "job_description_length": len(job_description_text),
            "analysis": analysis,
            "ai_provider": "google_gemini",
            "features": ["file_upload_support", "enhanced_skill_categorization", "requirement_extraction"],
            "message": f"Job description file '{file.filename}' analyzed successfully{'with AI enhancement' if enhance_with_ai else 'without enhancement'}"
        }

        # Add enhancement information if available
        if "enhancement_details" in analysis:
            result["enhancement_info"] = {
                "was_enhanced": analysis["enhancement_details"]["was_enhanced"],
                "optimization_applied": enhance_with_ai
            }

            if analysis["enhancement_details"]["was_enhanced"]:
                result["enhancement_info"]["improvements"] = analysis["enhancement_details"].get("optimization_notes")
                result["enhancement_info"]["quality_boost"] = analysis["enhancement_details"].get("quality_improvement")

        # Add file processing metadata
        result["file_processing"] = {
            "original_filename": file.filename,
            "file_size_kb": round(len(job_description_text) / 1024, 2),
            "extracted_title": job_title,
            "processing_successful": True
        }

        logger.info(f"‚úÖ Job description file analysis completed: {file.filename}")
        return result

    except HTTPException:
        # Re-raise HTTP exceptions as-is
        raise
    except Exception as e:
        logger.error(f"‚ùå Error analyzing job description file {file.filename}: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Error analyzing job description file: {str(e)}"
        )

@router.post("/analyze-resume-text")
async def analyze_resume_text(
    request: ResumeTextRequest
) -> Dict[str, Any]:
    """
    Analyze resume text and extract structured information using Google Gemini
    
    This endpoint:
    1. Takes resume text content
    2. Uses Google Gemini to extract candidate information
    3. Returns structured candidate profile with enhanced categorization
    """
    
    logger.info("üìã Analyzing resume text with Google Gemini")
    
    try:
        if not request.resume_text.strip():
            raise HTTPException(
                status_code=400,
                detail="Resume text cannot be empty"
            )
        
        # Analyze with Google Gemini
        analysis = await ai_analyzer.analyze_resume_content(
            request.resume_text,
            request.job_context
        )
        
        result = {
            "status": "success",
            "analysis": analysis,
            "text_length": len(request.resume_text),
            "ai_provider": "google_gemini", 
            "features": ["skill_categorization", "experience_analysis", "career_insights", "resume_quality_scoring"],
            "message": "Resume analyzed successfully with enhanced Google Gemini"
        }
        
        logger.info("‚úÖ Enhanced resume analysis completed successfully")
        return result
        
    except Exception as e:
        logger.error(f"‚ùå Error analyzing resume: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Error analyzing resume: {str(e)}"
        )

@router.get("/ai-status")
async def get_ai_status():
    """Check Google Gemini AI service status and test connection"""
    
    logger.info("üîç Checking enhanced Google Gemini AI status")
    
    # Test Google Gemini connection
    connection_test = await ai_analyzer.test_connection()
    
    return {
        "current_provider": "google_gemini",
        "google_api_configured": bool(settings.google_api_key),
        "max_tokens": settings.max_tokens,
        "temperature": settings.temperature,
        "model": "gemini-2.0-flash-exp",
        "connection_test": connection_test,
        "status": "ready" if connection_test.get("status") == "connected" else "not_ready",
        "new_features": [
            "job_description_enhancement",
            "job_description_file_upload",
            "pdf_docx_parsing",
            "ai_prompt_optimization",
            "improved_skill_categorization",
            "career_trajectory_analysis",
            "resume_quality_scoring"
        ],
        "supported_file_formats": {
            "job_descriptions": [".pdf", ".doc", ".docx"],
            "resumes": [".pdf", ".doc", ".docx"],
            "max_file_sizes": {
                "job_descriptions": "20MB",
                "resumes": "10MB"
            }
        }
    }

@router.post("/compare-job-descriptions")
async def compare_job_descriptions(
    request: Dict[str, Any]
) -> Dict[str, Any]:
    """
    üåü NEW: Compare multiple job descriptions
    
    This endpoint:
    1. Takes multiple job descriptions
    2. Enhances each one
    3. Provides comparison analysis
    4. Shows similarities and differences
    """
    
    logger.info("üîÑ Comparing multiple job descriptions")
    
    try:
        job_descriptions = request.get("job_descriptions", [])
        
        if len(job_descriptions) < 2:
            raise HTTPException(
                status_code=400,
                detail="At least 2 job descriptions required for comparison"
            )
        
        if len(job_descriptions) > 5:
            raise HTTPException(
                status_code=400,
                detail="Maximum 5 job descriptions allowed for comparison"
            )
        
        # Enhance and analyze each job description
        analyzed_jobs = []
        for i, job_desc in enumerate(job_descriptions):
            title = job_desc.get("title", f"Job {i+1}")
            description = job_desc.get("description", "")
            
            if not description.strip():
                continue
                
            analysis = await ai_analyzer.analyze_job_description(
                description, 
                title, 
                use_enhancement=True
            )
            
            analyzed_jobs.append({
                "title": title,
                "analysis": analysis,
                "index": i + 1
            })
        
        # Generate comparison insights
        comparison_result = {
            "status": "success",
            "total_jobs_analyzed": len(analyzed_jobs),
            "jobs": analyzed_jobs,
            "comparison_insights": {
                "common_skills": _find_common_skills(analyzed_jobs),
                "experience_range": _analyze_experience_requirements(analyzed_jobs),
                "education_patterns": _analyze_education_requirements(analyzed_jobs),
                "seniority_distribution": _analyze_seniority_levels(analyzed_jobs)
            },
            "ai_provider": "google_gemini",
            "message": f"Successfully compared {len(analyzed_jobs)} job descriptions"
        }
        
        logger.info(f"‚úÖ Job comparison completed for {len(analyzed_jobs)} positions")
        return comparison_result
        
    except Exception as e:
        logger.error(f"‚ùå Error comparing job descriptions: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Error comparing job descriptions: {str(e)}"
        )

# Helper functions for job comparison
def _find_common_skills(analyzed_jobs: list) -> Dict[str, Any]:
    """Find skills that appear across multiple job descriptions"""
    
    all_skills = {}
    total_jobs = len(analyzed_jobs)
    
    for job in analyzed_jobs:
        job_skills = job["analysis"].get("required_skills", {})
        for category, skills in job_skills.items():
            if not isinstance(skills, list):
                continue
                
            for skill in skills:
                skill_key = f"{category}:{skill}"
                if skill_key not in all_skills:
                    all_skills[skill_key] = 0
                all_skills[skill_key] += 1
    
    # Find skills that appear in 50%+ of jobs
    common_threshold = max(1, total_jobs // 2)
    common_skills = {
        skill.split(":", 1)[1]: count 
        for skill, count in all_skills.items() 
        if count >= common_threshold
    }
    
    return {
        "skills": common_skills,
        "threshold": f"Appears in {common_threshold}+ out of {total_jobs} jobs"
    }

def _analyze_experience_requirements(analyzed_jobs: list) -> Dict[str, Any]:
    """Analyze experience requirements across jobs"""
    
    experience_levels = []
    for job in analyzed_jobs:
        min_exp = job["analysis"].get("minimum_experience")
        if min_exp is not None:
            experience_levels.append(min_exp)
    
    if not experience_levels:
        return {"message": "No experience requirements specified"}
    
    return {
        "min_experience": min(experience_levels),
        "max_experience": max(experience_levels),
        "average_experience": sum(experience_levels) / len(experience_levels),
        "jobs_with_requirements": len(experience_levels),
        "total_jobs": len(analyzed_jobs)
    }

def _analyze_education_requirements(analyzed_jobs: list) -> Dict[str, Any]:
    """Analyze education requirements across jobs"""
    
    degree_requirements = []
    for job in analyzed_jobs:
        edu_req = job["analysis"].get("education_requirements", {})
        required_degree = edu_req.get("required_degree")
        if required_degree:
            degree_requirements.append(required_degree)
    
    degree_counts = {}
    for degree in degree_requirements:
        degree_counts[degree] = degree_counts.get(degree, 0) + 1
    
    return {
        "degree_distribution": degree_counts,
        "most_common": max(degree_counts.keys(), key=degree_counts.get) if degree_counts else None,
        "jobs_with_requirements": len(degree_requirements),
        "total_jobs": len(analyzed_jobs)
    }

def _analyze_seniority_levels(analyzed_jobs: list) -> Dict[str, Any]:
    """Analyze seniority levels across jobs"""
    
    seniority_levels = []
    for job in analyzed_jobs:
        level = job["analysis"].get("seniority_level")
        if level:
            seniority_levels.append(level)
    
    level_counts = {}
    for level in seniority_levels:
        level_counts[level] = level_counts.get(level, 0) + 1
    
    return {
        "level_distribution": level_counts,
        "most_common": max(level_counts.keys(), key=level_counts.get) if level_counts else None,
        "jobs_with_levels": len(seniority_levels),
        "total_jobs": len(analyzed_jobs)
    }